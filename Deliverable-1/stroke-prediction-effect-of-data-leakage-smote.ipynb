{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1 style=\"font-family: Times New Roman ; border-radius : 10px;padding: 20px; font-size: 40px; color: #DF6589; text-align: center; line-height: 0.55;background-color: #3C1053\"><b>Stroke Prediction</b><br></h1>","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://www.cdc.gov/stroke/images/Index-About.jpg?_=67208\" alt=\"Stroke Prediction\" width=\"50%\">\n</center>\n\n### Problem Statement :\n\nAccording to the World Health Organization (WHO), stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. It is another health issue that has found to be rising throughout the world due to the adoption of lifestyle changes that disregards healthy lifestyle & good eating habits. Thus, new emerging electronic devices that record the health vitals have paved the way for creating an automated solution with AI techniques at it's core. Thus, similar to heart diseases, efforts have begun to create lab tests that predict stroke. The dataset presented here has many factors that highlight the lifestyle of the patients and hence gives us an opportunity to create an AI-based solution for it.\n\n### Aim :\n- To classify / predict whether a patient can suffer a stroke.\n- It is a **binary classification** problem with multiple numerical and categorical features.","metadata":{}},{"cell_type":"markdown","source":"### Dataset Attributes :\n    \n- **id** : unique identifier\n- **gender** : \"Male\", \"Female\" or \"Other\"\n- **age** : age of the patient\n- **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n- **heart_disease** : 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n- **ever_married** : \"No\" or \"Yes\"\n- **work_type** : \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n- **Residence_type** : \"Rural\" or \"Urban\"\n- **avg_glucose_level** : average glucose level in blood\n- **bmi** : body mass index\n- **smoking_status** : \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n- **stroke** : 1 if the patient had a stroke or 0 if not","metadata":{}},{"cell_type":"markdown","source":"### Notebook Contents :\n- Dataset Information\n- Exploratory Data Analysis (EDA)\n- Summary of EDA & Comparison with Domain Information\n- Feature Engineering (Data Leakage & Data Balancing)\n- Modeling\n- Conclusion\n\n### What you will learn :\n- Data Visualization\n- Data Balancing using SMOTE\n- Data Leakage\n- Statistical Tests for Feature Selection\n- Modeling and visualization of results for algorithms\n\n### Lets get started!","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Dataset Information</div></center>","metadata":{}},{"cell_type":"markdown","source":"### Import the Necessary Libraries :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.float_format = '{:.2f}'.format\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:17.707447Z","iopub.execute_input":"2022-12-01T14:02:17.70817Z","iopub.status.idle":"2022-12-01T14:02:18.252171Z","shell.execute_reply.started":"2022-12-01T14:02:17.708097Z","shell.execute_reply":"2022-12-01T14:02:18.250904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-01T14:02:18.25976Z","iopub.execute_input":"2022-12-01T14:02:18.260494Z","iopub.status.idle":"2022-12-01T14:02:18.299245Z","shell.execute_reply.started":"2022-12-01T14:02:18.260445Z","shell.execute_reply":"2022-12-01T14:02:18.297868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Info :","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.300759Z","iopub.execute_input":"2022-12-01T14:02:18.301526Z","iopub.status.idle":"2022-12-01T14:02:18.3082Z","shell.execute_reply.started":"2022-12-01T14:02:18.301484Z","shell.execute_reply":"2022-12-01T14:02:18.307153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.311312Z","iopub.execute_input":"2022-12-01T14:02:18.311707Z","iopub.status.idle":"2022-12-01T14:02:18.319293Z","shell.execute_reply.started":"2022-12-01T14:02:18.311675Z","shell.execute_reply":"2022-12-01T14:02:18.318055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.320921Z","iopub.execute_input":"2022-12-01T14:02:18.321289Z","iopub.status.idle":"2022-12-01T14:02:18.340959Z","shell.execute_reply.started":"2022-12-01T14:02:18.321257Z","shell.execute_reply":"2022-12-01T14:02:18.339799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(data.isnull(),cmap = 'magma',cbar = False);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.34224Z","iopub.execute_input":"2022-12-01T14:02:18.342553Z","iopub.status.idle":"2022-12-01T14:02:18.772542Z","shell.execute_reply.started":"2022-12-01T14:02:18.342523Z","shell.execute_reply":"2022-12-01T14:02:18.771567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **A few null values** are present in the **bmi** feature!","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.773879Z","iopub.execute_input":"2022-12-01T14:02:18.774754Z","iopub.status.idle":"2022-12-01T14:02:18.805831Z","shell.execute_reply.started":"2022-12-01T14:02:18.774717Z","shell.execute_reply":"2022-12-01T14:02:18.804968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stroke = data[data['stroke'] == 1].describe().T\nno_stroke = data[data['stroke'] == 0].describe().T\n\ncolors = ['#3C1053','#DF6589']\n\nfig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))\nplt.subplot(1,2,1)\nsns.heatmap(stroke[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\nplt.title('Stroke Suffered');\n\nplt.subplot(1,2,2)\nsns.heatmap(no_stroke[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\nplt.title('No Stroke Suffered');\n\nfig.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:18.807241Z","iopub.execute_input":"2022-12-01T14:02:18.807835Z","iopub.status.idle":"2022-12-01T14:02:19.245065Z","shell.execute_reply.started":"2022-12-01T14:02:18.807802Z","shell.execute_reply":"2022-12-01T14:02:19.244065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Mean** values of all the features for cases of **stroke suffered** and **no stroke suffered**.\n- **age** and **avg_glucose_level** can be solid 1st hand indicators to identify a stroke.\n- Mean **age** values of patients that suffered a stroke, **67.73**,is much higher than those did not suffer a stroke, **41.97**.\n- Similarly, **avg_glucose_level** value of **132.54** can indicate a higher chance of suffering from stroke than the **avg_glucose_level** value of **104.80** that has been found in patients that did not suffer a stroke.","metadata":{}},{"cell_type":"markdown","source":"### Fill Missing Values :","metadata":{}},{"cell_type":"code","source":"l1 = [i for i in tqdm(range(len(data.isnull()['bmi']))) if data.isnull().loc[i,'bmi'] == True]\nprint('Total Number of Missing Values in bmi feature :', len(l1))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:19.246659Z","iopub.execute_input":"2022-12-01T14:02:19.246987Z","iopub.status.idle":"2022-12-01T14:02:26.399731Z","shell.execute_reply.started":"2022-12-01T14:02:19.246958Z","shell.execute_reply":"2022-12-01T14:02:26.398667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the table of descriptive statistics, we observe that mean and median values of **bmi** are very close to each other.\n- Hence, we will fill the missing values with the **mean values**.","metadata":{}},{"cell_type":"code","source":"data['bmi'].fillna(data['bmi'].mean(),inplace = True)\nsns.heatmap(data.isnull(),cmap = 'magma',cbar = False);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:26.401277Z","iopub.execute_input":"2022-12-01T14:02:26.402043Z","iopub.status.idle":"2022-12-01T14:02:26.818946Z","shell.execute_reply.started":"2022-12-01T14:02:26.402006Z","shell.execute_reply":"2022-12-01T14:02:26.817965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Exploratory Data Analysis</div></center>","metadata":{}},{"cell_type":"markdown","source":"### Dividing features into Discrete and Categorical :","metadata":{}},{"cell_type":"code","source":"data.drop(columns = ['id'],inplace = True)\ncol = list(data.columns)\ncategorical_features = []\ndiscrete_features = []\nfor i in col:\n    if len(data[i].unique()) > 6:\n        discrete_features.append(i)\n    else:\n        categorical_features.append(i)\n\nprint('Categorical Features :',*categorical_features)\nprint('Discrete Features :',*discrete_features)\n\ndata['age'] = data['age'].astype(int)\ndf1 = data.copy(deep = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:26.820328Z","iopub.execute_input":"2022-12-01T14:02:26.821271Z","iopub.status.idle":"2022-12-01T14:02:26.836681Z","shell.execute_reply.started":"2022-12-01T14:02:26.821231Z","shell.execute_reply":"2022-12-01T14:02:26.83528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-  We drop the **id** column as it is just a unique identifier.\n- Here, categorical features are defined if the the attribute has less than 6 unique elements else it is a discrete feature.\n- Typical approach for this division of features can also be based on the datatypes of the elements of the respective attribute.\n\n**Eg :** datatype = integer, attribute = discrete feature ; datatype = string, attribute = categorical feature\n\n- Creating a deep copy of the orginal dataset for experimenting with data, visualization and modeling.\n- Modifications in the original dataset will not be highlighted in this deep copy.\n- We now Label Encode the data categorical text data features. ","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ntext_data_features = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\nl3 = []; l4 = [];\nprint('Label Encoder Transformation')\nfor i in tqdm(text_data_features):\n    df1[i] = le.fit_transform(df1[i])\n    l3.append(list(df1[i].unique())); l4.append(list(le.inverse_transform(df1[i].unique())))\n    print(i,' : ',df1[i].unique(),' = ',le.inverse_transform(df1[i].unique()))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:26.838555Z","iopub.execute_input":"2022-12-01T14:02:26.83942Z","iopub.status.idle":"2022-12-01T14:02:26.87041Z","shell.execute_reply.started":"2022-12-01T14:02:26.839368Z","shell.execute_reply":"2022-12-01T14:02:26.869249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We store the label encoded transformations inside a dictionary that gives us the information about the encoded value and it's original value! \n- We add the remaining 2 features manually i.e **heart_disease** & **hypertension**!","metadata":{}},{"cell_type":"code","source":"tf1 = {}\nfor i in range(len(text_data_features)):\n    tf1[text_data_features[i]] = {}\n    for j,k in zip(l3[i],l4[i]):\n        tf1[text_data_features[i]][j] = k\n\ntf1['hypertension'] = {0 : 'No Hypertension', 1 : 'Hypertension'} \ntf1['heart_disease'] = {0 : 'No Heart Disease', 1 : 'Heart Disease'}\ntf1","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:26.876134Z","iopub.execute_input":"2022-12-01T14:02:26.876572Z","iopub.status.idle":"2022-12-01T14:02:26.887983Z","shell.execute_reply.started":"2022-12-01T14:02:26.876535Z","shell.execute_reply":"2022-12-01T14:02:26.886737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Variable Visualization (stroke) : ","metadata":{}},{"cell_type":"code","source":"l = list(df1['stroke'].value_counts())\ncircle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n\nfig = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\nplt.subplot(1,2,1)\nplt.pie(circle,labels = ['No Stroke Suffered','Stroke Suffered'],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors,\n       wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\nplt.title('Stroke Events (%)');\n\nplt.subplot(1,2,2)\nax = sns.countplot('stroke',data = df1, palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['No Stroke Suffered','Stroke Suffered'])\nplt.title('Number of Stroke Events');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:26.889647Z","iopub.execute_input":"2022-12-01T14:02:26.89007Z","iopub.status.idle":"2022-12-01T14:02:27.155218Z","shell.execute_reply.started":"2022-12-01T14:02:26.890035Z","shell.execute_reply":"2022-12-01T14:02:27.154108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Clearly, the dataset is unbalanced in the favour of **no stroke**.\n- **19 : 1** ratio is observed for **No Stroke : Stroke!**\n- Thus, due to such heavy bias towards cases of **No Stroke**, predictions cannot be trusted!","metadata":{}},{"cell_type":"markdown","source":"### Discrete Features :\n\n#### Distribution of Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1,ncols = 3,figsize = (20,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    sns.distplot(df1[discrete_features[i]],color = colors[0])\n    title = 'Distribution : ' + discrete_features[i]\n    plt.title(title)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:27.157035Z","iopub.execute_input":"2022-12-01T14:02:27.157458Z","iopub.status.idle":"2022-12-01T14:02:27.889129Z","shell.execute_reply.started":"2022-12-01T14:02:27.157413Z","shell.execute_reply":"2022-12-01T14:02:27.887955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Data distribution for **age** has dominant values around : **10**, **60** & **80**.\n- **avg_glucose_level** has 2 peaks of uneven heights present at values around : **100** & **200**.\n- **bmi** has a near about **normal distribution** but it has values in low numbers towards the right side! ","metadata":{}},{"cell_type":"markdown","source":"### Discrete Features w.r.t Target Variable (stroke) :","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 3,ncols = 1,figsize = (15,15))\nfor i in range(len(discrete_features)):\n    plt.subplot(3,1,i+1)\n    sns.countplot(discrete_features[i],data = df1,hue = \"stroke\",palette = colors,edgecolor = 'black')\n    plt.legend(['No Stroke Suffered', 'Stroke Suffered'] ,loc = 'upper right')\n    title = discrete_features[i] + ' w.r.t stroke'\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:02:27.890581Z","iopub.execute_input":"2022-12-01T14:02:27.8915Z","iopub.status.idle":"2022-12-01T14:03:38.90717Z","shell.execute_reply.started":"2022-12-01T14:02:27.891454Z","shell.execute_reply":"2022-12-01T14:03:38.905726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Because of too many unique data points in the **discrete_features**, it is difficult to gain any type of insight. Thus, we will convert these features into categorical features for visualizations.\n- We scale the data points of these features to a constant value that represents a range of values.(like mean)\n- Here, we divide the data points by a constant value and assign it's quotient value as the representative constant. The scaling constants are decided by looking into the data & intuition. ","metadata":{}},{"cell_type":"code","source":"df1['age_group'] = [ int(i / 5) for i in df1['age']]\ndf1['avg_glucose_level_group'] = [ int(i / 20) for i in df1['avg_glucose_level']]\ndf1['bmi_group'] = [ int(i / 5) for i in df1['bmi']]","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:38.90933Z","iopub.execute_input":"2022-12-01T14:03:38.909807Z","iopub.status.idle":"2022-12-01T14:03:38.92927Z","shell.execute_reply.started":"2022-12-01T14:03:38.909745Z","shell.execute_reply":"2022-12-01T14:03:38.92772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 3,ncols = 1,figsize = (15,15))\ngroup_numerical_features = [i + '_group' for i in ['age','avg_glucose_level','bmi']]\n\nfor i in range(len(group_numerical_features)):\n    plt.subplot(3,1,i+1)\n    sns.countplot(group_numerical_features[i],data = df1,hue = \"stroke\",palette = colors,edgecolor = 'black')\n    plt.legend(['No Stroke Suffered', 'Stroke Suffered'] ,loc = 'upper right')\n    title = group_numerical_features[i] + ' w.r.t stroke'\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:38.931545Z","iopub.execute_input":"2022-12-01T14:03:38.93213Z","iopub.status.idle":"2022-12-01T14:03:39.914977Z","shell.execute_reply.started":"2022-12-01T14:03:38.932015Z","shell.execute_reply":"2022-12-01T14:03:39.91384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For **age**, cases of **stroke suffered** can be observed for the values between **35**(7x5) - **80**(16x5). It is not a dominant patch due to the imbalance nature of the dataset.  \n- For **avg_glucose_level**, 2 groups can be found : **60**(3x20) - **100**(5x20) & **180**(9x20) - **220**(11x20). Patients with **avg_glucose_level** present in the 1st group are more prone to suffering **stroke** than group 2.\n- **bmi** values from **15**(3x5) - **40**(8x5) have displayed more cases of **stroke**.","metadata":{}},{"cell_type":"markdown","source":"### Categorical Features :\n\n#### Distribution of Categorical Features :","metadata":{}},{"cell_type":"code","source":"categorical_features.remove('stroke')","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:39.916365Z","iopub.execute_input":"2022-12-01T14:03:39.916687Z","iopub.status.idle":"2022-12-01T14:03:39.922693Z","shell.execute_reply.started":"2022-12-01T14:03:39.916657Z","shell.execute_reply":"2022-12-01T14:03:39.921485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We remove the **stroke** feature from the list of categorical features as it is the target variable and we will treat it separately!","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 2,ncols = 2,figsize = (7,9))\nfor i in range(len(categorical_features[:4])):\n    plt.subplot(2,2,i+1)\n    sns.distplot(df1[categorical_features[i]],kde_kws = {'bw' : 1},color = colors[0]);\n    title = 'Distribution : ' + categorical_features[i]\n    plt.title(title)\n    \nfig,ax = plt.subplots(nrows = 1,ncols = 3,figsize = (15,3))\nfor i in range(-1,-4,-1):\n    plt.subplot(1,3,-i)\n    sns.distplot(df1[categorical_features[i]],kde_kws = {'bw' : 1},color = colors[0]);\n    title = 'Distribution : ' + categorical_features[i]\n    plt.title(title)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:39.924196Z","iopub.execute_input":"2022-12-01T14:03:39.92452Z","iopub.status.idle":"2022-12-01T14:03:41.365279Z","shell.execute_reply.started":"2022-12-01T14:03:39.92449Z","shell.execute_reply":"2022-12-01T14:03:41.364053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All the categorical features are **Normally Distributed**.","metadata":{}},{"cell_type":"markdown","source":"### Categorical Features w.r.t Target Variable (stroke) :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2,ncols = 2,figsize = (15,10))\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    ax = sns.countplot(categorical_features[i],data = df1,hue = \"stroke\",palette = colors,edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels([tf1[categorical_features[i]][j] for j in sorted(df1[categorical_features[i]].unique())])\n    plt.legend(['No Stroke Suffered', 'Stroke Suffered'], loc = 'upper right')\n    title = categorical_features[i] + ' w.r.t stroke'\n    plt.title(title);\n\nfig = plt.subplots(nrows = 1,ncols = 3,figsize = (15,5))\nfor i in range(-1,-4,-1):\n    plt.subplot(1,3,-i)\n    ax = sns.countplot(categorical_features[i],data = df1,hue = \"stroke\",palette = colors,edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels([tf1[categorical_features[i]][j] for j in sorted(df1[categorical_features[i]].unique())])\n    plt.legend(['No Stroke Suffered', 'Stroke Suffered'], loc = 'upper right')\n    title = categorical_features[i] + ' w.r.t stroke'\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:41.367316Z","iopub.execute_input":"2022-12-01T14:03:41.368138Z","iopub.status.idle":"2022-12-01T14:03:42.782868Z","shell.execute_reply.started":"2022-12-01T14:03:41.36808Z","shell.execute_reply":"2022-12-01T14:03:42.781569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All the graphs near about share the same pattern i.e displaying low number of **stroke** cases and no clear cut reason to point towards.  \n- **Female** population has recorded more cases of **stroke** than **male**.\n- Interestingly, people with **no hypertension** & **no heart disease** have displayed to be more prone to **suffering stroke** than people that have these medical conditions.\n- According to the dataset, people that have been **married** have **suffered stroke** more than those people who have never married.\n- When it comes to **smoking_status**, people that have **never smoked** have topped the numbers with **formerly smoked** people coming at the 2nd position to record **stroke** cases.\n- Not much info can be gained from **Residence_type** & **work_type**, however **Private** workers **suffered stroke** cases more than any other worker.","metadata":{}},{"cell_type":"markdown","source":"### Categorical Features w.r.t Positive Cases of stroke :","metadata":{}},{"cell_type":"code","source":"gender = df1[df1['stroke'] == 1]['gender'].value_counts()\ngender = [gender[0] / sum(gender) * 100, gender[1] / sum(gender) * 100]\n\nhypertension = df1[df1['stroke'] == 1]['hypertension'].value_counts()\nhypertension = [hypertension[0] / sum(hypertension) * 100, hypertension[1] / sum(hypertension) * 100]\n\nheart_disease = df1[df1['stroke'] == 1]['heart_disease'].value_counts()\nheart_disease = [heart_disease[0] / sum(heart_disease) * 100, heart_disease[1] / sum(heart_disease) * 100]\n\never_married = df1[df1['stroke'] == 1]['ever_married'].value_counts()\never_married = [ever_married[0] / sum(ever_married) * 100, ever_married[1] / sum(ever_married) * 100]\n\nwork_type = df1[df1['stroke'] == 1]['work_type'].value_counts()\nwork_type = [work_type[0] / sum(work_type) * 100, work_type[2] / sum(work_type) * 100,\n             work_type[3] / sum(work_type) * 100, work_type[4] / sum(work_type) * 100,]\n\nResidence_type = df1[df1['stroke'] == 1]['Residence_type'].value_counts()\nResidence_type = [Residence_type[0] / sum(Residence_type) * 100, Residence_type[1] / sum(Residence_type) * 100]\n\nsmoking_status = df1[df1['stroke'] == 1]['smoking_status'].value_counts()\nsmoking_status = [smoking_status[0] / sum(smoking_status) * 100, smoking_status[1] / sum(smoking_status) * 100,\n                  smoking_status[2] / sum(smoking_status) * 100, smoking_status[3] / sum(smoking_status) * 100]","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:42.784541Z","iopub.execute_input":"2022-12-01T14:03:42.785019Z","iopub.status.idle":"2022-12-01T14:03:42.808304Z","shell.execute_reply.started":"2022-12-01T14:03:42.784974Z","shell.execute_reply":"2022-12-01T14:03:42.807173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l1 = [gender, hypertension, heart_disease, ever_married, work_type, Residence_type, smoking_status]\n\nax,fig = plt.subplots(nrows = 2,ncols = 2,figsize = (10,10))\nfor i in range(4):\n    if len(l1[i]) == 2:\n        plt.subplot(2,2,i + 1)\n        plt.pie(l1[i],labels = [tf1[categorical_features[i]][j] for j in sorted(df1[df1['stroke'] == 1][categorical_features[i]].unique())],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors,\n               wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n        plt.title(categorical_features[i]);\n    else:\n        plt.subplot(2,2,i + 1)\n        plt.pie(l1[i],labels = [tf1[categorical_features[i]][j] for j in sorted(df1[df1['stroke'] == 1][categorical_features[i]].unique())],autopct='%1.1f%%',startangle = 90,explode = (0.1,0,0.1,0),colors = colors,\n               wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n        plt.title(categorical_features[i]);\n\nax,fig = plt.subplots(nrows = 1,ncols = 3,figsize = (15,15))\nfor i in range(-1,-4,-1):\n    if len(l1[i]) == 2:\n        plt.subplot(1,3,-i)\n        plt.pie(l1[i],labels = [tf1[categorical_features[i]][j] for j in sorted(df1[df1['stroke'] == 1][categorical_features[i]].unique())],autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors,\n               wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n        plt.title(categorical_features[i]);\n    else:\n        plt.subplot(1,3,-i)\n        plt.pie(l1[i],labels = [tf1[categorical_features[i]][j] for j in sorted(df1[df1['stroke'] == 1][categorical_features[i]].unique())],autopct='%1.1f%%',startangle = 90,explode = (0.1,0,0.1,0),colors = colors,\n               wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n        plt.title(categorical_features[i]);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:42.80949Z","iopub.execute_input":"2022-12-01T14:03:42.809862Z","iopub.status.idle":"2022-12-01T14:03:43.492011Z","shell.execute_reply.started":"2022-12-01T14:03:42.809827Z","shell.execute_reply":"2022-12-01T14:03:43.490715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- According to the data, **female** population is more susceptible to **suffering stroke**.\n- From the above visuals, surprisingly, **stroke** cases were found more in patients that **did not have any medical conditions like heart_disease or hypertension**!\n- People that have been **married** have the highest probability of suffering from **stroke**. \n- When it comes to **stroke & smoking_status**, people that have **never smoked** are most susceptible. They are followed by **formerly smoked**. Surprisingly, people that **smokes** have the lowest chances of **suffering from stroke**.\n- People living in **Urban Residence Type** have edged the people living in **Rural Residence Type** in terms of **stroke** cases.\n- When it comes to **work_type**, people working in **Private** have recorded more than **50%+** of **stroke** cases. It is followed by **Self-employed** & **Govt_job** that record half and quarter of the cases that **Private** recorderd respectively.","metadata":{}},{"cell_type":"markdown","source":"### Categorical features vs Discrete features w.r.t Target variable (stroke) :","metadata":{}},{"cell_type":"markdown","source":"#### gender vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'gender',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['gender'][j] for j in sorted(df1['gender'].unique())])\n    title = discrete_features[i] + ' vs gender'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:43.494101Z","iopub.execute_input":"2022-12-01T14:03:43.494956Z","iopub.status.idle":"2022-12-01T14:03:44.136Z","shell.execute_reply.started":"2022-12-01T14:03:43.494905Z","shell.execute_reply":"2022-12-01T14:03:44.135164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For both **male** & **female** population, **age** of those **suffering from stroke** is **60+**.\n- For majority of the **avg_glucose_level** values, both **gender** have recorded significant cases of **stroke**.\n- For **male** population, the lower limit of **bmi** values is slightly higher than the **female**. Overall, both the **gender** overlap the same **bmi** values for cases of **stroke**.","metadata":{}},{"cell_type":"markdown","source":"#### hypertension vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'hypertension',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['hypertension'][j] for j in sorted(df1['hypertension'].unique())])\n    title = discrete_features[i] + ' vs hypertension'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:44.137428Z","iopub.execute_input":"2022-12-01T14:03:44.13803Z","iopub.status.idle":"2022-12-01T14:03:44.736743Z","shell.execute_reply.started":"2022-12-01T14:03:44.137995Z","shell.execute_reply":"2022-12-01T14:03:44.735379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Wierdly, cases of **stroke** found in people having **hypertension** have a high lower limit of the **age 60+** than those who do not suffer from **hypertension**.\n- When it comes to **hypertension & avg_glucose_level**, cases of **stroke** & **no stroke** near about share the same values.\n- Due to **hypertension**, lower limits of **bmi** values are slightly reduced making people prone to **stroke**. ","metadata":{}},{"cell_type":"markdown","source":"#### heart_disease vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'heart_disease',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['heart_disease'][j] for j in sorted(df1['heart_disease'].unique())])\n    title = discrete_features[i] + ' vs heart_disease'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:44.738418Z","iopub.execute_input":"2022-12-01T14:03:44.738848Z","iopub.status.idle":"2022-12-01T14:03:45.301987Z","shell.execute_reply.started":"2022-12-01T14:03:44.738814Z","shell.execute_reply":"2022-12-01T14:03:45.300758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Graphs of **hypertension** & **heart_disease** against discrete features are very similar with slight differences.\n- They share the same effects on **stroke**.","metadata":{}},{"cell_type":"markdown","source":"#### ever_married vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'ever_married',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['ever_married'][j] for j in sorted(df1['ever_married'].unique())])\n    title = discrete_features[i] + ' vs ever_married'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:45.303762Z","iopub.execute_input":"2022-12-01T14:03:45.304494Z","iopub.status.idle":"2022-12-01T14:03:45.857812Z","shell.execute_reply.started":"2022-12-01T14:03:45.304445Z","shell.execute_reply":"2022-12-01T14:03:45.85668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For **ever_married vs discrete features**, repeated insights can be found.\n- People that have been **married** have displayed cases of **stroke** for near about all the values of **avg_glucose_level**.","metadata":{}},{"cell_type":"markdown","source":"#### work_type vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'work_type',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['work_type'][j] for j in sorted(df1['work_type'].unique())])\n    title = discrete_features[i] + ' vs work_type'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:45.859304Z","iopub.execute_input":"2022-12-01T14:03:45.860748Z","iopub.status.idle":"2022-12-01T14:03:46.688943Z","shell.execute_reply.started":"2022-12-01T14:03:45.860709Z","shell.execute_reply":"2022-12-01T14:03:46.688007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Irrespective of the **work_type**, **stroke** cases have been found for **age of 60+** except for **children**.\n- Clearly, people that have worked to earn a living have suffered from **stroke**. \n- **Stroke** cases have been found more in people working in a job i.e **Govt_job** & **Private** than those who are **Self-employed**.","metadata":{}},{"cell_type":"markdown","source":"#### Residence_type vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'Residence_type',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['Residence_type'][j] for j in sorted(df1['Residence_type'].unique())])\n    title = discrete_features[i] + ' vs Residence_type'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:46.690035Z","iopub.execute_input":"2022-12-01T14:03:46.690953Z","iopub.status.idle":"2022-12-01T14:03:47.279885Z","shell.execute_reply.started":"2022-12-01T14:03:46.690915Z","shell.execute_reply":"2022-12-01T14:03:47.279018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The graphs of **Rural Residence_type** & **Urban Residence_type** against discrete features w.r.t **stroke** are identical.\n- They cannot be separated from each other. They repeat the insights that have been highlighted uptill now.","metadata":{}},{"cell_type":"markdown","source":"#### smoking_status vs Discrete Features :","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nfor i in range(len(discrete_features)):\n    plt.subplot(1,3,i+1)\n    ax = sns.boxplot(x = 'smoking_status',y = discrete_features[i],data = df1,hue = 'stroke',palette = colors);\n    ax.set_xticklabels([tf1['smoking_status'][j] for j in sorted(df1['smoking_status'].unique())])\n    title = discrete_features[i] + ' vs smoking_status'\n    plt.legend(['No Stroke','Stroke'], loc = 'upper right')\n    plt.title(title);","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:47.281017Z","iopub.execute_input":"2022-12-01T14:03:47.281933Z","iopub.status.idle":"2022-12-01T14:03:48.052404Z","shell.execute_reply.started":"2022-12-01T14:03:47.281897Z","shell.execute_reply":"2022-12-01T14:03:48.051254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Irrespective of **smoking_status**, people **suffering from stroke** have been detected at **age around 60**.\n- Similar to **age**, same values of **avg_glucose_level** have been found in **stroke** cases irrespective of **smoking_status**.\n- However, because of the **smoking_status**, range of values for which cases of **stroke** differ. Range of values of people that **smokes** is slightly higher than everyone else.","metadata":{}},{"cell_type":"markdown","source":"### Discrete features vs Discrete features w.r.t Target variable (stroke) :","metadata":{}},{"cell_type":"code","source":"a = 0\nfig,ax = plt.subplots(nrows = 1,ncols = 3,figsize = (15,5),squeeze = False)\nfor i in range(len(discrete_features) - 1):\n    for j in range(len(discrete_features)):\n        if i != j and j > i:\n            a += 1\n            plt.subplot(1,3,a)\n            sns.scatterplot(x = discrete_features[i],y = discrete_features[j],data = df1,hue = 'stroke',palette = colors,edgecolor = 'black');\n            title = discrete_features[i] + ' vs ' + discrete_features[j]\n            plt.legend(['No Stroke','Stroke'], loc = 'upper right',)\n            plt.title(title)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-01T14:03:48.053981Z","iopub.execute_input":"2022-12-01T14:03:48.054347Z","iopub.status.idle":"2022-12-01T14:03:49.340888Z","shell.execute_reply.started":"2022-12-01T14:03:48.054313Z","shell.execute_reply":"2022-12-01T14:03:49.339703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Due to the imbalance nature of the data, cases of **stroke** & **no stroke** cannot be separated.\n- No insights can be interpreted from the above graphs.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Summary of EDA</div></center>\n\n### Order / Values of features for positive cases of stroke :\n\n- **Categorical Features (Order) :**\n    \n    - **gender** : female > male\n    - **hypertension** : no hypertension > hypertension\n    - **heart_disease** : no heart disease > heart disease\n    - **ever_married** : married > no married\n    - **working_type** : Private > Self-employed > Govt_job > children \n    - **Residence_type** : Urban > Rural\n    - **smoking_status** : never smoked > formerly smoked > smokes\n\n\n- **Discrete Features (Range) :**\n    \n    - **age** : 55 - 80 \n    - **avg_glucose_level** : 80 - 200\n    - **bmi** : 20 - 40\n\n\n- **According to the data, these order / range of values leads to heart failures.**","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Domain Information</div></center>\n\n- **Categorical Features (Order) :**\n    \n    - **gender** : male > female\n    - **hypertension** : hypertension > no hypertension\n    - **heart_disease** : heart disease > no heart disease\n    - **ever_married** : married = not married \n    - **working_type** : Stress of the work can lead to stroke.\n    - **Residence_type** : Mortality due to stroke is higher in rural areas than urban areas due to poor medical treatment. \n    - **smoking_status** : Smoking increases the risk of stroke.\n\n\n- **Discrete Features (Range) :**\n    \n    - **age** : The chance of having a stroke about doubles every 10 years after age 55. \n    - **avg_glucose_level** : High blood glucose is found in stroke cases. A value of 126+ has been observed alot. \n    - **bmi** : High bmi values increases the chances of ischemic stroke.\n\n\n- All the information mentioned is gathered from websites and research papers. We will use this information for cross checking the summary of EDA and feature selection.\n\n\n- Conclusions obtained from the EDA contradict the Domain Information for the features : **hypertension**, **heart_disease** and **smoking_status**.\n\n\n- This difference is probably because of the **Unbalanced dataset**!\n\n\n- Thus, we will carry out the feature engineering process, balance the dataset using **SMOTE analysis** and feed the balanced to the ML algorithms.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Feature Engineering</div></center>","metadata":{}},{"cell_type":"markdown","source":"- The dataset is **Unbalanced** with a bias towards **No Stroke** in a ratio of **19 : 1** for **No Stroke : Stroke**. We will first balance the dataset using **SMOTE Analysis**!\n\n- In order to cope with unbalanced data, there are 2 options :\n\n    - **Undersampling** : Trim down the majority samples of the target variable.\n    - **Oversampling** : Increase the minority samples of the target variable to the majority samples.\n    \n- For best performances, combination of undersampling and oversampling is recommended.\n- First, we will undersample the majority samples and it is followed by oversampling minority samples.\n- For data balancing, we will use **imblearn**.\n- **PIP statement** : pip install imbalanced-learn","metadata":{}},{"cell_type":"markdown","source":"### Data Balancing using SMOTE :","metadata":{}},{"cell_type":"code","source":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:49.34232Z","iopub.execute_input":"2022-12-01T14:03:49.34268Z","iopub.status.idle":"2022-12-01T14:03:49.724311Z","shell.execute_reply.started":"2022-12-01T14:03:49.342638Z","shell.execute_reply":"2022-12-01T14:03:49.723177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"over = SMOTE(sampling_strategy = 1)\nunder = RandomUnderSampler(sampling_strategy = 0.1)\nf1 = df1.loc[:,:'smoking_status']\nt1 = df1.loc[:,'stroke']\n\nsteps = [('under', under),('over', over)]\npipeline = Pipeline(steps=steps)\nf1, t1 = pipeline.fit_resample(f1, t1)\nCounter(t1)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:49.725726Z","iopub.execute_input":"2022-12-01T14:03:49.726945Z","iopub.status.idle":"2022-12-01T14:03:49.754484Z","shell.execute_reply.started":"2022-12-01T14:03:49.726904Z","shell.execute_reply":"2022-12-01T14:03:49.753648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculation for Data Balancing :\n\n- **Sampling Strategy** : It is a ratio which is the common paramter for oversampling and undersampling.\n- **Sampling Strategy** : **( Samples of Minority Class ) / ( Samples of Majority Class )**\n\n\n- In this case,\n\n    - **Majority Class : No Stroke** : 4861 samples\n    - **Minority Class : Stroke** : 249 samples\n\n\n### Undersampling : Trim down the majority class samples\n\n- Sampling_Strategy = 0.1\n- 0.1 = ( 249 ) / Majority Class Samples\n- After undersampling, \n\n    - **Majority Class : No Stroke** : 2490 samples\n    - **Minority Class : Stroke** : 249 samples\n\n\n### Oversampling : Increase the minority class samples\n\n- Sampling_Strategy = 1\n- 1 = ( Minority Class Samples ) / 2490\n- After oversampling, \n\n    - **Majority Class : No Stroke** : 2490 samples\n    - **Minority Class : Stroke** : 2490 samples\n    \n\n- Final Class Samples :\n\n    - **Majority Class : No Stroke** : 2490 samples\n    - **Minority Class : Stroke** : 2490 samples\n\n\n- Here, we balance the data by reducing the majority group samples & then increasing the minority group to majority group. \n- For imbalanced datasets, we **duplicate the data** to deal with the potential bias in the predictions. \n- Due to this duplication process, we are using **synthetic data** for modeling purposes to ensure that the predictions are not skewed towards the majority target class value.\n- Thus, evaluating models using **accuracy** will be misleading. Instead, we will go for **confusion matrix, ROC-AUC graph and ROC-AUC score** for model evaluation.","metadata":{}},{"cell_type":"markdown","source":"### Data Leakage : \n\n- **Data Leakage** is the problem when the information outside the training data is used for model creation. It is one of the most ignored problem.\n- In order to create robust models, solving data leakage is a must! Creation of overly optimistic models which are practically useless & cannot be used in production have become common.\n- Model performance degrades when **Data Leakage** is not dealt with & the model is sent online. It is a difficult concept to understand because it seems quite trivial.\n- Typical approach used is transforming / modifying the entire dataset by filling NAN values with mean, median & mode, standardisation, normalization, etc.\n- When we execute the above process in order to make the dataset ready for modeling, we use the values from the entire dataset & thus indirectly provide information from the **to-be test data** i.e outside of the training data.\n- Thus, in order to avoid **Data Leakage**, it is advised to use **train-test-split** before any transformations. Execute the transformations according to the training data for the training as well as test data. Use of k-fold cross validation is also suggested!\n- We will display the effect of **Data Leakage** from the following code snippet!","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(f1, t1, test_size = 0.15, random_state = 2)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:49.755691Z","iopub.execute_input":"2022-12-01T14:03:49.756051Z","iopub.status.idle":"2022-12-01T14:03:49.764838Z","shell.execute_reply.started":"2022-12-01T14:03:49.756017Z","shell.execute_reply":"2022-12-01T14:03:49.763514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrix :","metadata":{}},{"cell_type":"code","source":"x_train_test = x_train.copy(deep = True)\nx_train_test['stroke'] = y_train","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:49.766298Z","iopub.execute_input":"2022-12-01T14:03:49.766658Z","iopub.status.idle":"2022-12-01T14:03:49.782362Z","shell.execute_reply.started":"2022-12-01T14:03:49.766623Z","shell.execute_reply":"2022-12-01T14:03:49.781256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In order to visualize the correlation matrix, we create a new dataframe that contains values from **x_train** & **y_train**.\n- Thus, we reject anything outside the training data to avoid data leakage.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n\nplt.subplot(1,2,1)\ncorr = x_train_test.corrwith(x_train_test['stroke']).sort_values(ascending = False).to_frame()\ncorr.columns = ['stroke']\nsns.heatmap(corr,annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black');\nplt.title('Correlation w.r.t stroke : No Data Leakage');\n\nplt.subplot(1,2,2)\ncorr = df1.drop(columns = ['age_group', 'avg_glucose_level_group', 'bmi_group']).corrwith(df1['stroke']).sort_values(ascending = False).to_frame()\ncorr.columns = ['stroke']\nsns.heatmap(corr,annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black');\nplt.title('Correlation w.r.t stroke : Data Leakage');","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:49.784221Z","iopub.execute_input":"2022-12-01T14:03:49.784557Z","iopub.status.idle":"2022-12-01T14:03:50.474743Z","shell.execute_reply.started":"2022-12-01T14:03:49.784528Z","shell.execute_reply":"2022-12-01T14:03:50.47352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Clearly, we can see the difference in values between **Data Leakage** & **No Data Leakage**.\n- In the case of **No Data Leakage**, **age** displays a strong positive correlation with **stroke**. **avg_glucose_level** & **ever_married** display some kind of positive correlation. Opposite to positive correlation, **gender**, **Residence_type** & **work_type** have negative correlation with the **stroke**.\n- In the case of **Data Leakage**, none of the features display an extreme positive or negative correlation with **stroke**.\n- **age**, **heart_disease**, **avg_glucose_level**, **hypertension** & **ever_married** display some kind of positive correlation. Overall, all the features have a value very close to 0, displaying neutral correlation with **stroke**.","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection for Categorical Features :","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif,chi2","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:50.476461Z","iopub.execute_input":"2022-12-01T14:03:50.476877Z","iopub.status.idle":"2022-12-01T14:03:50.492094Z","shell.execute_reply.started":"2022-12-01T14:03:50.47684Z","shell.execute_reply":"2022-12-01T14:03:50.490984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mutual Information Test :","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n\nplt.subplot(1,2,1)\nfeatures = x_train.loc[:,categorical_features]\ntarget = pd.DataFrame(y_train)\n\nbest_features = SelectKBest(score_func = mutual_info_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Mutual Information Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Mutual Information Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features : No Data Leakage');\n\nplt.subplot(1,2,2)\nfeatures = df1.loc[:,categorical_features]\ntarget = df1.loc[:,'stroke']\n\nbest_features = SelectKBest(score_func = mutual_info_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Mutual Information Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Mutual Information Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features : Data Leakage');","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:50.493497Z","iopub.execute_input":"2022-12-01T14:03:50.494374Z","iopub.status.idle":"2022-12-01T14:03:51.399752Z","shell.execute_reply.started":"2022-12-01T14:03:50.494337Z","shell.execute_reply":"2022-12-01T14:03:51.398791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Mutual Information Score of **stroke** with categorical features display very low scores irrespective of **Data Leakage** or **No Data Leakage**.\n- According to the above scores, none of the features should be selected for modeling.","metadata":{}},{"cell_type":"markdown","source":"#### Chi Squared Test :","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n\nplt.subplot(1,2,1)\nfeatures = x_train.loc[:,categorical_features]\ntarget = y_train\n\nbest_features = SelectKBest(score_func = chi2,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Chi Squared Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Chi Squared Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features : No Data Leakage');\n\nplt.subplot(1,2,2)\nfeatures = df1.loc[:,categorical_features]\ntarget = df1.loc[:,'stroke']\n\nbest_features = SelectKBest(score_func = chi2,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Chi Squared Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Chi Squared Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features : Data Leakage');","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:51.401206Z","iopub.execute_input":"2022-12-01T14:03:51.401811Z","iopub.status.idle":"2022-12-01T14:03:52.014031Z","shell.execute_reply.started":"2022-12-01T14:03:51.401761Z","shell.execute_reply":"2022-12-01T14:03:52.013148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For **No Data Leakage**, we should reject the features that have low values. We will reject features with scores less than 20. Hence, we will not use : **smoking_status**, **heart_disease** & **hypertension**. This does contradict with the **Domain Information**.\n- For **Data Leakage**, **heart disease** & **hypertension** need to be selected for modeling and reject the other features due to low Chi Squared Score. ","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection for Numerical Features :","metadata":{}},{"cell_type":"markdown","source":"#### ANOVA Test :","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n\nplt.subplot(1,2,1)\nfeatures = x_train.loc[:,discrete_features]\ntarget = y_train\n\nbest_features = SelectKBest(score_func = f_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['ANOVA Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'ANOVA Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Numerical Features : No Data Leakage');\n\nplt.subplot(1,2,2)\nfeatures = df1.loc[:,discrete_features]\ntarget = df1.loc[:, 'stroke']\n\nbest_features = SelectKBest(score_func = f_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['ANOVA Score']) \nsns.heatmap(featureScores.sort_values(ascending = False,by = 'ANOVA Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Numerical Features : Data Leakage');","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:52.020648Z","iopub.execute_input":"2022-12-01T14:03:52.021471Z","iopub.status.idle":"2022-12-01T14:03:53.381685Z","shell.execute_reply.started":"2022-12-01T14:03:52.021416Z","shell.execute_reply":"2022-12-01T14:03:53.380295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above ANOVA Scores, we ignore the features with values less than 20. Hence, we reject **bmi** for modeling irrespective of **Data Leakage** or **No Data Leakage**.\n- We ready the datasets for data scaling by dropping the features based on the above statistical tests.\n- We will ignore the **Domain Information**!","metadata":{}},{"cell_type":"code","source":"# Feature Selection for No Data Leakage :\nx_train = x_train.drop(columns = ['smoking_status', 'heart_disease', 'hypertension', 'bmi'])\nx_test = x_test.drop(columns = ['smoking_status', 'heart_disease', 'hypertension', 'bmi'])\n\n# Feature Selection for Data Leakage :\ndf2 = df1.drop(columns = ['smoking_status', 'work_type', 'Residence_type', 'gender', \n                          'bmi', 'age_group', 'avg_glucose_level_group', 'bmi_group']).copy(deep = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.383338Z","iopub.execute_input":"2022-12-01T14:03:53.383671Z","iopub.status.idle":"2022-12-01T14:03:53.392373Z","shell.execute_reply.started":"2022-12-01T14:03:53.383641Z","shell.execute_reply":"2022-12-01T14:03:53.391004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling :","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\nmms = MinMaxScaler() # Normalization\nss = StandardScaler() # Standardization\n\n# No Data Leakage :\n\n# Normalization\nx_train['age'] = mms.fit_transform(x_train[['age']])\nx_test['age'] = mms.transform(x_test[['age']])\nx_train['avg_glucose_level'] = mms.fit_transform(x_train[['avg_glucose_level']])\nx_test['avg_glucose_level'] = mms.transform(x_test[['avg_glucose_level']])\n\n# Standardization\nx_train['gender'] = ss.fit_transform(x_train[['gender']]); x_test['gender'] = ss.transform(x_test[['gender']])\nx_train['ever_married'] = ss.fit_transform(x_train[['ever_married']]); x_test['ever_married'] = ss.transform(x_test[['ever_married']])\nx_train['work_type'] = ss.fit_transform(x_train[['work_type']]); x_test['work_type'] = ss.transform(x_test[['work_type']])\nx_train['Residence_type'] = ss.fit_transform(x_train[['Residence_type']]); x_test['Residence_type'] = ss.transform(x_test[['Residence_type']])\n\n# Data Leakage :\n\n# Normalization\ndf2['age'] = mms.fit_transform(df2[['age']])\ndf2['avg_glucose_level'] = mms.fit_transform(df2[['avg_glucose_level']])\n\n# Standardization\ndf2['hypertension'] = mms.fit_transform(df2[['hypertension']])\ndf2['heart_disease'] = mms.fit_transform(df2[['heart_disease']])\ndf2['ever_married'] = mms.fit_transform(df2[['ever_married']])","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.393695Z","iopub.execute_input":"2022-12-01T14:03:53.394093Z","iopub.status.idle":"2022-12-01T14:03:53.461234Z","shell.execute_reply.started":"2022-12-01T14:03:53.39406Z","shell.execute_reply":"2022-12-01T14:03:53.460057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Machine learning model does not understand the units of the values of the features. It treats the input just as a simple number but does not understand the true meaning of that value. Thus, it becomes necessary to scale the data.\n\n- We have 2 options for data scaling : \n    \n    1) **Normalization** \n    \n    2) **Standardization**. \n\n\n- As most of the algorithms assume the data to be normally (Gaussian) distributed, **Normalization** is done for features whose data does not display normal distribution and **standardization** is carried out for features that are normally distributed but the range of values is huge or small as compared to other features.\n\n- From the above transformation, we fit the data on the training data and transform the test data from information based on the training data. If we check the formulas of the **Normalization** & **Standardization**, we use mean, standard deviation, min & max values.\n\n- Thus if these above statistical parameters are calculated using the complete dataset, then we are sharing the values from the **to-be test data** and thus sharing this **to-be test data** with the training data and cause **Data Leakage**.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Modeling</div></center>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import precision_recall_curve","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.462898Z","iopub.execute_input":"2022-12-01T14:03:53.463688Z","iopub.status.idle":"2022-12-01T14:03:53.46967Z","shell.execute_reply.started":"2022-12-01T14:03:53.463645Z","shell.execute_reply":"2022-12-01T14:03:53.468192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train1, x_test1, y_train1, y_test1 = x_train.values, x_test.values, y_train.values, y_test.values\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(df2.drop(columns = 'stroke').values, df2['stroke'].values, test_size = 0.15, random_state = 2)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.470903Z","iopub.execute_input":"2022-12-01T14:03:53.471466Z","iopub.status.idle":"2022-12-01T14:03:53.485383Z","shell.execute_reply.started":"2022-12-01T14:03:53.471431Z","shell.execute_reply":"2022-12-01T14:03:53.484176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Selecting the features from the above conducted tests and splitting the data into **85 - 15 train - test** groups.","metadata":{}},{"cell_type":"code","source":"def model(classifier,x_train,y_train,x_test,y_test):\n    \n    classifier.fit(x_train,y_train)\n    prediction = classifier.predict(x_test)\n    cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)\n    print(\"Cross Validation Score : \",'{0:.2%}'.format(cross_val_score(classifier,x_train,y_train,cv = cv,scoring = 'roc_auc').mean()))\n    print(\"ROC_AUC Score : \",'{0:.2%}'.format(roc_auc_score(y_test,prediction)))\n    plot_roc_curve(classifier, x_test,y_test)\n    plt.title('ROC_AUC_Plot')\n    plt.show()\n\ndef model_evaluation(classifier,x_test,y_test):\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test,classifier.predict(x_test))\n    names = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [value for value in cm.flatten()]\n    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cm,annot = labels,cmap = colors,fmt ='')\n    \n    # Classification Report\n    print(classification_report(y_test,classifier.predict(x_test)))","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.486792Z","iopub.execute_input":"2022-12-01T14:03:53.487203Z","iopub.status.idle":"2022-12-01T14:03:53.498961Z","shell.execute_reply.started":"2022-12-01T14:03:53.487172Z","shell.execute_reply":"2022-12-01T14:03:53.498041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1] XGBoostClassifier :","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.500197Z","iopub.execute_input":"2022-12-01T14:03:53.501039Z","iopub.status.idle":"2022-12-01T14:03:53.594977Z","shell.execute_reply.started":"2022-12-01T14:03:53.500996Z","shell.execute_reply":"2022-12-01T14:03:53.593946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_xgb = XGBClassifier(learning_rate= 0.01,max_depth = 3,n_estimators = 1000)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.596399Z","iopub.execute_input":"2022-12-01T14:03:53.5967Z","iopub.status.idle":"2022-12-01T14:03:53.60156Z","shell.execute_reply.started":"2022-12-01T14:03:53.596671Z","shell.execute_reply":"2022-12-01T14:03:53.600534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model for No Data Leakage :","metadata":{}},{"cell_type":"code","source":"model(classifier_xgb,x_train1,y_train1,x_test1,y_test1)\nmodel_evaluation(classifier_xgb,x_test1,y_test1)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:03:53.602917Z","iopub.execute_input":"2022-12-01T14:03:53.603245Z","iopub.status.idle":"2022-12-01T14:06:25.573468Z","shell.execute_reply.started":"2022-12-01T14:03:53.603215Z","shell.execute_reply":"2022-12-01T14:06:25.572599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model for Data Leakage :","metadata":{}},{"cell_type":"code","source":"model(classifier_xgb,x_train2,y_train2,x_test2,y_test2)\nmodel_evaluation(classifier_xgb,x_test2,y_test2)","metadata":{"execution":{"iopub.status.busy":"2022-12-01T14:06:25.575162Z","iopub.execute_input":"2022-12-01T14:06:25.575874Z","iopub.status.idle":"2022-12-01T14:08:53.788397Z","shell.execute_reply.started":"2022-12-01T14:06:25.575824Z","shell.execute_reply":"2022-12-01T14:08:53.786427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ML Alogrithm Results Table :\n\n#### No Data Leakage Model : \n\n|Sr. No.|ML Algorithm|Cross Validation Score|ROC AUC Score|F1 Score (Stroke)|F1 Score (No Stroke)|\n|-|-|-|-|-|-|\n|1|XGB Classifier|91.45%|83.03%|84%|82%|\n\n#### Data Leakage Model :\n\n\n|Sr. No.|ML Algorithm|Cross Validation Score|ROC AUC Score|F1 Score (Stroke)|F1 Score (No Stroke)|\n|-|-|-|-|-|-|\n|1|XGB Classifier|83.53%|50%|0%|97%|","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Conclusion</div></center>\n\n- This is another good dataset for understanding how to handle binary classification problems however, imbalance nature of the dataset makes it a tricky task to deal with.\n\n\n- It's EDA opportunities are immense as well. However, it's EDA insights did contradict with the domain knowledge. \n\n\n- In order to understand the significance of **Data Leakage** & it's effects, 2 models are created. You can easily spot the difference in statistical test values, feature selections and machine learning model performance. \n\n\n- F1 score of **No Data Leakage Model** suggests that it is more robust and can deal with the unknown data better than the **Data Leakage Model**.","metadata":{}},{"cell_type":"markdown","source":"### References :\n- https://www.youtube.com/watch?v=n9jz7G68pVg\n- https://machinelearningmastery.com/data-preparation-without-data-leakage/#:~:text=Data%20leakage%20refers%20to%20a,a%20marked%20effect%20on%20performance.\n- https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data\n- [Image Source](https://www.cdc.gov/stroke/images/Index-About.jpg?_=67208)","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Please upvote if you like the work!</div><div style=\"font-family: Times New Roman; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Any Sort of Feedback is Appreciated!</div><div style=\"font-family: Times New Roman; background-color: #3C1053; color: #DF6589; padding: 12px; line-height: 1;\">Thank You!</div></center>","metadata":{}}]}